{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7f10b19-c061-42b2-8b42-e3cbafa3b1da",
      "metadata": {
        "id": "d7f10b19-c061-42b2-8b42-e3cbafa3b1da"
      },
      "source": [
        "# Fine-Tune FLAN-T5 with Reinforcement Learning (PPO) and PEFT to Generate Less-Toxic Summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ef668a-9c51-489b-be47-a07a09ef2289",
      "metadata": {
        "id": "36ef668a-9c51-489b-be47-a07a09ef2289"
      },
      "source": [
        "이 노트북에서는, Meta AI's hate speech reward model를 사용해 덜 유해한 콘텐츠를 생성하도록 FLAN-T5모델을 파인튜닝하게 됩니다. 보상 모델은 \"hate\"와 \"not hate\"를 예측하는 이진 분류기 입니다. Proximal Policy Optimization(PPO)를 파인튜닝에 사용하고 모델의 유해성을 줄일 예정입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5003e2-a642-416b-bd3f-93fb339c3a7d",
      "metadata": {
        "tags": [],
        "id": "ed5003e2-a642-416b-bd3f-93fb339c3a7d"
      },
      "source": [
        "# Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6791f449-d1da-461b-9eb7-c6dc6b37b15c",
      "metadata": {
        "tags": [],
        "id": "6791f449-d1da-461b-9eb7-c6dc6b37b15c"
      },
      "source": [
        "- [ 1 - Set up Kernel and Required Dependencies](#1)\n",
        "- [ 2 - Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator](#2)\n",
        "  - [ 2.1 - Load Data and FLAN-T5 Model Fine-Tuned with Summarization Instruction](#2.1)\n",
        "  - [ 2.2 - Prepare Reward Model](#2.2)\n",
        "  - [ 2.3 - Evaluate Toxicity](#2.3)\n",
        "- [ 3 - Perform Fine-Tuning to Detoxify the Summaries](#3)\n",
        "  - [ 3.1 - Initialize `PPOTrainer`](#3.1)\n",
        "  - [ 3.2 - Fine-Tune the Model](#3.2)\n",
        "  - [ 3.3 - Evaluate the Model Quantitatively](#3.3)\n",
        "  - [ 3.4 - Evaluate the Model Qualitatively](#3.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89f973f1-f095-4915-86d0-bc16380da22d",
      "metadata": {
        "tags": [],
        "id": "89f973f1-f095-4915-86d0-bc16380da22d"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Set up Kernel and Required Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c0b396-8dab-4937-907e-734df9e2442c",
      "metadata": {
        "tags": [],
        "id": "43c0b396-8dab-4937-907e-734df9e2442c"
      },
      "source": [
        "LLM과 데이터셋에 필요한 패키지를 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f9d24e86-f76f-4a44-90ef-0777752075a8",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9d24e86-f76f-4a44-90ef-0777752075a8",
        "outputId": "0ea20490-e997-426d-9ba3-ab24487f6757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/lvwerra/trl.git@25fa1bd\n",
            "  Cloning https://github.com/lvwerra/trl.git (to revision 25fa1bd) to /tmp/pip-req-build-y8sbeoyc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-req-build-y8sbeoyc\n",
            "\u001b[33m  WARNING: Did not find branch or tag '25fa1bd', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running command git checkout -q 25fa1bd\n",
            "  Resolved https://github.com/lvwerra/trl.git to commit 25fa1bd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: transformers>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (4.37.2)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (1.23.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (0.26.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (2.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (4.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (0.42.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl==0.4.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (10.0.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (0.70.14)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets->trl==0.4.2.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (3.9.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (0.18.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.2.dev0) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.4.2.dev0) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    datasets==2.11.0 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    peft==0.3.0 --quiet\n",
        "\n",
        "# Installing the Reinforcement Learning library directly from github.\n",
        "%pip install git+https://github.com/lvwerra/trl.git@25fa1bd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74bfd06e-c747-43e0-b86c-0398628e1c32",
      "metadata": {
        "tags": [],
        "id": "74bfd06e-c747-43e0-b86c-0398628e1c32"
      },
      "source": [
        "필요한 요소를 Import 합니다. 새로운 요소가 있는데, 뒤에서 다루도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d8c20bed-6a30-4847-a507-02969ecb4465",
      "metadata": {
        "tags": [],
        "id": "d8c20bed-6a30-4847-a507-02969ecb4465"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
        "\n",
        "# trl: Transformer Reinforcement Learning library\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
        "from trl import create_reference_model\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# tqdm library makes the loops show a smart progress meter.\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "SAmv99lBe1sh"
      },
      "id": "SAmv99lBe1sh",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b76eea84-8e3a-4487-9692-613977e6c8e3",
      "metadata": {
        "id": "b76eea84-8e3a-4487-9692-613977e6c8e3"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5f97d4-ea5f-4072-b5d6-785d1d833ed4",
      "metadata": {
        "tags": [],
        "id": "4a5f97d4-ea5f-4072-b5d6-785d1d833ed4"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 - Load Data and FLAN-T5 Model Fine-Tuned with Summarization Instruction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90dc0211-4032-4967-946d-3a538829d5c9",
      "metadata": {
        "tags": [],
        "id": "90dc0211-4032-4967-946d-3a538829d5c9"
      },
      "source": [
        "이전과 동일하게, 계속해서 Hugging Face dataset [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum)과 the pre-trained model [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)를 사용합니다.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b058b52b-ec4d-4426-8d71-91e898f727f6",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "6c993d5729454e17b1b29c573bdd4659",
            "99552bcab45a4960b6d282464aaf8b8b",
            "502a7a256a274679bd97c5e643bd89d3",
            "16afa8a93ab7490dbe65786f40fc9c2c",
            "1084b824ea3d41788ddcf3f68b98f8d5",
            "17b5847c91ce4bc685e83fc0cbd6017b",
            "a6bfdbba237b4b84a61741cdd2391e51",
            "0127ba120185403d95186badd8bca8e9",
            "2e02e24572bf43d983e32a3ec1a22c4c",
            "991c019b3a944781a4346f9bfd54d0e4",
            "622778d041d445ac8a84d8d247adbc18"
          ]
        },
        "id": "b058b52b-ec4d-4426-8d71-91e898f727f6",
        "outputId": "39e8b23d-fc70-49ee-c01f-685f915979fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c993d5729454e17b1b29c573bdd4659"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model_name='marianna13/flan-t5-base-summarization'\n",
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "dataset_original = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "dataset_original"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "668d30d6-6f81-4e52-a81a-3057163ddb0e",
      "metadata": {
        "id": "668d30d6-6f81-4e52-a81a-3057163ddb0e"
      },
      "source": [
        "다음으로 데이터셋을 전처리합니다. 특정한 길이의 담화만 필터링해 전체의 일부만 사용합니다(충분히 길고 읽기 쉽게 만들기 위해). 이후, Instruction에 맞춰 각 담화를 감싸고, 토큰화합니다. 토큰 ids는 `input_ids`에 저장하고, 디코딩된 버전은 `query`에 저장합니다.\n",
        "\n",
        "단계적으로 아래 셀에서 수행할 수 있지만, `build_dataset`이라는 함수로 구성하는 것이 더 좋은 습관입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "51469abe-4d72-4093-a6c6-8e04e19f09eb",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231,
          "referenced_widgets": [
            "e31dc262b56e4477b45d0c061d6eff3e",
            "28bbea489fb74c3fa0267608bfc9ced4",
            "de204bc9054748a2870b6281a66a8685",
            "a915ceeb8d7e400abc750507fee157ee",
            "f27e927b4b93403e9d4b05969b41ba2e",
            "3d3850cea0ad42739638abdeb1594f52",
            "92670892e37240afac79d892c02f6bf4",
            "eede8b2fca5a4fc9b6d8980c2bfba99c",
            "0f77cfeeac644c36b6e919ee34032165",
            "643f0a1a9c0849cc997e176ce19d005c",
            "662a47db42264feaaeb94d4f4640ae75"
          ]
        },
        "id": "51469abe-4d72-4093-a6c6-8e04e19f09eb",
        "outputId": "920fd4a8-d909-4125-deb3-078d99963ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-23525d6e58aeae0c.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10022 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e31dc262b56e4477b45d0c061d6eff3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
            "        num_rows: 8017\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
            "        num_rows: 2005\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def build_dataset(model_name,\n",
        "                  dataset_name,\n",
        "                  input_min_text_length,\n",
        "                  input_max_text_length):\n",
        "\n",
        "    \"\"\"\n",
        "    Preprocess the dataset and split it into train and test parts.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): Tokenizer model name.\n",
        "    - dataset_name (str): Name of the dataset to load.\n",
        "    - input_min_text_length (int): Minimum length of the dialogues.\n",
        "    - input_max_text_length (int): Maximum length of the dialogues.\n",
        "\n",
        "    Returns:\n",
        "    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n",
        "    \"\"\"\n",
        "\n",
        "    # load dataset (only \"train\" part will be enough for this lab).\n",
        "    dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n",
        "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "\n",
        "    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "    def tokenize(sample):\n",
        "\n",
        "        # Wrap each dialogue with the instruction.\n",
        "        prompt = f\"\"\"\n",
        "          Summarize the following conversation.\n",
        "\n",
        "          {sample[\"dialogue\"]}\n",
        "\n",
        "          Summary:\n",
        "          \"\"\"\n",
        "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
        "\n",
        "        # This must be called \"query\", which is a requirement of our PPO library.\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    # Tokenize each dialogue.\n",
        "    dataset = dataset.map(tokenize, batched=False)\n",
        "    dataset.set_format(type=\"torch\")\n",
        "\n",
        "    # Split the dataset into train and test parts.\n",
        "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
        "\n",
        "    return dataset_splits\n",
        "\n",
        "dataset = build_dataset(model_name=model_name,\n",
        "                        dataset_name=huggingface_dataset_name,\n",
        "                        input_min_text_length=200,\n",
        "                        input_max_text_length=1000)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d03155e-649b-45bb-a5a0-94edd682c069",
      "metadata": {
        "tags": [],
        "id": "7d03155e-649b-45bb-a5a0-94edd682c069"
      },
      "source": [
        "이전 랩에서, 데이터의 일부분을 사용하여, 요약 instruction과 함께 PEFT 모델을 파인튜닝 했습니다. 그리고 S3로부터 모든 데이터를 활용해 학습된 PEFT 모델을 불러왔던 것을 기억할 것 입니다.\n",
        "\n",
        "여기서는 Colab을 활용하기에 이미 Summerization에 대해 Full Finetuning된 모델을 불러오겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4226923-67c0-4ea6-8e47-030136b2f191",
      "metadata": {
        "id": "f4226923-67c0-4ea6-8e47-030136b2f191"
      },
      "source": [
        "Prepare a function to pull out the number of model parameters (it is the same as in the previous lab):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a1f06806-a194-4c14-b64d-e31afd7b658c",
      "metadata": {
        "tags": [],
        "id": "a1f06806-a194-4c14-b64d-e31afd7b658c"
      },
      "outputs": [],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e06a57-fb80-4f8c-a967-4c7c42a7bfda",
      "metadata": {
        "tags": [],
        "id": "21e06a57-fb80-4f8c-a967-4c7c42a7bfda"
      },
      "source": [
        "FLAN-T5모델의 어뎁터를 추가합니다. 이전의 랩에서는 인퍼런스 타임에만 fully trained adapter을 더했기 때문에 LoRA 설정을 전달할 필요가 없었지만, 이번에는 `is_trainable=True`와 함께 전달해줍시다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "metadata": {
        "id": "cPdEA3gH_iGv"
      },
      "id": "cPdEA3gH_iGv",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a1a94b14-b375-45e7-9e49-a7f2c341b4ff",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1a94b14-b375-45e7-9e49-a7f2c341b4ff",
        "outputId": "c7d40743-954c-4a41-ba1d-3655d98ec82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEFT model parameters to be updated:\n",
            "\n",
            "trainable model parameters: 3538944\n",
            "all model parameters: 251116800\n",
            "percentage of trainable model parameters: 1.41%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
        "                                              torch_dtype=torch.bfloat16)\n",
        "\n",
        "peft_model = get_peft_model(model=model,\n",
        "                            peft_config=lora_config)\n",
        "\n",
        "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a950ae8a-76b9-4951-9c78-9ac7a6349e17",
      "metadata": {
        "id": "a950ae8a-76b9-4951-9c78-9ac7a6349e17"
      },
      "source": [
        "이번 랩에서는, 강화학습(RL)으로 LLM을 파인튜닝하기 위해 준비합니다. RL은 간단하게 다음 차수의 랩에서 소개될 예정입니다. 이번 스테이지에서는 파인튜닝된 PEFT모델을 전달해서 간단히 PPO 모델을 준비하기만 하면됩니다. PPO는 reward model의 RL policy를 최적하는데 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1e86bab0-6dee-4dff-a754-b584ed962723",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e86bab0-6dee-4dff-a754-b584ed962723",
        "outputId": "4d816ae1-3a3c-4d55-ca2c-15c8629ea34d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPO model parameters to be updated (ValueHead + 769 params):\n",
            "\n",
            "trainable model parameters: 3539713\n",
            "all model parameters: 251117569\n",
            "percentage of trainable model parameters: 1.41%\n",
            "\n",
            "ValueHead(\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
        "                                                               torch_dtype=torch.bfloat16,\n",
        "                                                               is_trainable=True)\n",
        "\n",
        "print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
        "print(ppo_model.v_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2913ef05-737e-4cdf-9bac-467ee6cf9f76",
      "metadata": {
        "id": "2913ef05-737e-4cdf-9bac-467ee6cf9f76"
      },
      "source": [
        "PPO 동안, 매우 적은 파라미터만 업데이트됩니다. 특히, `ValueHead`의 파라미터를 업데이트합니다. 모델의 class에 대한 더 많은 정보는 여기 [documentation](https://huggingface.co/docs/trl/main/en/models#trl.create_reference_model)서 찾을 수 있습니다. 학습가능한 파라미터수는 $(n+1)*m$로 나타낼 수 있고, 여기서 $n$는 인풋 유닛의 수(여기서는 $n=768$)이고 $m$은 아웃풋 유닛의 수(여기서는 $m=1$)입니다. $+1$은 bias로 인해 계산된 항목입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c7e2df-0c75-4bd0-bf8d-1f1545ef864e",
      "metadata": {
        "id": "76c7e2df-0c75-4bd0-bf8d-1f1545ef864e"
      },
      "source": [
        "이제 레퍼런스 모델로 쓰일 파인튜닝 되지 않은 PPO의 frozen copy를 생성합니다. 이 레퍼런스 모델은 detoxification을 하지 않은 LLM을 나타냅니다. 이 레퍼런스 모델의 파라미터의 어느것도 PPO동안 학습되지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "18a9b30a-ad14-4189-8088-d4de447fe247",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18a9b30a-ad14-4189-8088-d4de447fe247",
        "outputId": "dbe30248-07ef-47fb-b868-9be374d17b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference model parameters to be updated:\n",
            "\n",
            "trainable model parameters: 0\n",
            "all model parameters: 251117569\n",
            "percentage of trainable model parameters: 0.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ref_model = create_reference_model(ppo_model)\n",
        "\n",
        "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a14848-e83d-4fdd-bc68-eb770c5951d7",
      "metadata": {
        "tags": [],
        "id": "e3a14848-e83d-4fdd-bc68-eb770c5951d7"
      },
      "source": [
        "모든 준비가 끝났습니다. 리워드 모델을 준비할 시간입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bfdf1f7-3509-4adc-812a-2b22bd330137",
      "metadata": {
        "id": "4bfdf1f7-3509-4adc-812a-2b22bd330137"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 - Prepare Reward Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**강화학습(RL)**은 누적된 보상을 최대화하는것을 목표로 주어진 환경에서 액션을 선택하도록하는 머신러닝의 종류 중 하나입니다. 강화학습의 목표는 에이전트가 **보상함수**를 최대화하는 optimal, 혹은 nearly-optimal, policy를 배우도록 하는 것입니다."
      ],
      "metadata": {
        "id": "0yUDWn34Lzme"
      },
      "id": "0yUDWn34Lzme"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[이전 섹션](#2.1)에서 오리지널 Policy는 instruct PEFT 모델을 바탕으로 합니다. 이것은 detoxification 전의 LLM입니다. 이후, 인간 라벨러에게 주어진 출력의 유해성을 피드백하도록 할 수 있지만 이러한 작업은 전체 파인튜닝 작업에 사용하면 매우 비쌉니다. 이러한 문제를 회피하는 실용적인 방법은 보상 모델을 사용해 에이전트가 담화를 detoxify하도록 장려하는 것입니다. 직관적인 접근으로 아마 두 개의 클래스(`nothate` and `hate`)에 대해 감성 분석하고 `nothate` 클래스의 확률이 높을 때 높은 보상을 주는 방법을 생각할 수 있습니다."
      ],
      "metadata": {
        "id": "dz1XAjkfMW-A"
      },
      "id": "dz1XAjkfMW-A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리는 [Meta AI's RoBERTa-based hate speech model](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target)을 보상 모델로 사용할 에정입니다. 이 모델은 **logits**를 반환하고 두 클래스: `nothate`와 `hate`에 대한 확률을 계산합니다. `nothate` 출력에 대한 logits을 보상으로 사용합니다. 이후, 이 보상값을 사용해 PPO로 파인튜닝 합니다."
      ],
      "metadata": {
        "id": "UwVhaPe1NZvi"
      },
      "id": "UwVhaPe1NZvi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "RoBERTa을 위한 모델 클래스의 인스턴스를 생성합니다. 또한 필요한 토크나이저를 로드합니다. `0`이 `nothate`이고, `1`이 `hate`라는 것을 잊지마세요."
      ],
      "metadata": {
        "id": "wOKHEfBxN-iS"
      },
      "id": "wOKHEfBxN-iS"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7f038a9f-e04b-49bc-8923-8ef3816919ee",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f038a9f-e04b-49bc-8923-8ef3816919ee",
        "outputId": "be1362a1-1721-4e20-f6b7-32a4a03ed591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'nothate', 1: 'hate'}\n"
          ]
        }
      ],
      "source": [
        "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
        "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
        "print(toxicity_model.config.id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d68799-a6e8-42d7-8d61-002e47210c18",
      "metadata": {
        "tags": [],
        "id": "79d68799-a6e8-42d7-8d61-002e47210c18"
      },
      "source": [
        "non-toxic text를 토큰화한 후 모델에 넣어보자. 출력 logit, 확률 그리고 파인튜닝에 사용할 상응하는 보상값을 출력하자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8f4e6a05-2398-4ca7-a176-d5a1ff27fe39",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f4e6a05-2398-4ca7-a176-d5a1ff27fe39",
        "outputId": "4dfb145f-f600-4e82-c5e7-772a09ce9861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits [not hate, hate]: [3.114100694656372, -2.4896175861358643]\n",
            "probabilities [not hate, hate]: [0.9963293671607971, 0.0036706167738884687]\n",
            "reward (high): [3.114100694656372]\n"
          ]
        }
      ],
      "source": [
        "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
        "\n",
        "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "logits = toxicity_model(input_ids=toxicity_input_ids.to(device)).logits\n",
        "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
        "\n",
        "# Print the probabilities for [not hate, hate]\n",
        "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "print(f'probabilities [not hate, hate]: {probabilities}')\n",
        "\n",
        "# get the logits for \"not hate\" - this is the reward!\n",
        "not_hate_index = 0\n",
        "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
        "print(f'reward (high): {nothate_reward}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f729c5-98c3-4745-96e8-3484670215db",
      "metadata": {
        "id": "63f729c5-98c3-4745-96e8-3484670215db"
      },
      "source": [
        "toxic한 text에 대해서도 해보세요. 이 텍스트는 더 유해하기 때문에, 더 낮은 보상을 보일 것 입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0ffc81e4-b220-4f4e-95ec-98ac418612d1",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ffc81e4-b220-4f4e-95ec-98ac418612d1",
        "outputId": "697095f3-f059-4c54-f6e7-b6079d717c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits [not hate, hate]: [-0.6921164393424988, 0.37227070331573486]\n",
            "probabilities [not hate, hate]: [0.2564719617366791, 0.7435280084609985]\n",
            "reward (low): [-0.6921164393424988]\n"
          ]
        }
      ],
      "source": [
        "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
        "\n",
        "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "logits = toxicity_model(toxicity_input_ids.to(device)).logits\n",
        "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
        "\n",
        "# Print the probabilities for [not hate, hate]\n",
        "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "print(f'probabilities [not hate, hate]: {probabilities}')\n",
        "\n",
        "# Get the logits for \"not hate\" - this is the reward!\n",
        "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
        "print(f'reward (low): {nothate_reward}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6e656e-fefe-4623-8bbb-472a8cf1c3c5",
      "metadata": {
        "id": "bc6e656e-fefe-4623-8bbb-472a8cf1c3c5"
      },
      "source": [
        "유해성 보상 모델을 위한 코드를 단순화하기 위해 허깅페이스 인퍼런스 파이프라인을 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "73aab8c6-33eb-4bf4-a816-3866fc3460af",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73aab8c6-33eb-4bf4-a816-3866fc3460af",
        "outputId": "072a8ceb-3f58-4e39-dbac-e746b47b3202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward model output:\n",
            "For non-toxic text\n",
            "[{'label': 'nothate', 'score': 3.114100694656372}, {'label': 'hate', 'score': -2.4896175861358643}]\n",
            "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.003670616541057825}]\n",
            "For toxic text\n",
            "[{'label': 'hate', 'score': 0.37227070331573486}, {'label': 'nothate', 'score': -0.6921164393424988}]\n",
            "[{'label': 'hate', 'score': 0.7435280084609985}, {'label': 'nothate', 'score': 0.25647199153900146}]\n"
          ]
        }
      ],
      "source": [
        "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
        "                          model=toxicity_model_name,\n",
        "                          device=device)\n",
        "reward_logits_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
        "    \"batch_size\": 16\n",
        "}\n",
        "\n",
        "reward_probabilities_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n",
        "    \"batch_size\": 16\n",
        "}\n",
        "\n",
        "print(\"Reward model output:\")\n",
        "print(\"For non-toxic text\")\n",
        "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
        "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
        "print(\"For toxic text\")\n",
        "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
        "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21302d74-59d8-451f-b287-e86245bf3324",
      "metadata": {
        "id": "21302d74-59d8-451f-b287-e86245bf3324"
      },
      "source": [
        "`nothate` (positive)와 `hate` (negative) 두 클래스에 대한 출력이 나옵니다. 하지만 PPO는 LLM의 출력을 detoxify하기 위해 `nothate` 클래스에 대한 logit만 positive reward 신호로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "36ff3925-70c4-495c-ae26-68e2fc36296b",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ff3925-70c4-495c-ae26-68e2fc36296b",
        "outputId": "2cfe0882-3ae4-4d2a-88ab-c728cdd108a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'nothate', 'score': 3.114100694656372}, {'label': 'hate', 'score': -2.4896175861358643}]\n",
            "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.003670616541057825}]\n"
          ]
        }
      ],
      "source": [
        "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
        "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8d11618b-5887-489a-b390-2139e364987f",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d11618b-5887-489a-b390-2139e364987f",
        "outputId": "e9343b66-4a4f-4e9c-a5d0-84bcfcafe66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'hate', 'score': 0.37227070331573486}, {'label': 'nothate', 'score': -0.6921164393424988}]\n",
            "[{'label': 'hate', 'score': 0.7435280084609985}, {'label': 'nothate', 'score': 0.25647199153900146}]\n"
          ]
        }
      ],
      "source": [
        "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
        "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56513033-9bb1-41d5-81e2-54d1249c5c89",
      "metadata": {
        "tags": [],
        "id": "56513033-9bb1-41d5-81e2-54d1249c5c89"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3 - Evaluate Toxicity\n",
        "\n",
        "fine-tuning/detoxification의 전후의 모델을 평가하기 위해서 [toxicity evaluation metric](https://huggingface.co/spaces/evaluate-measurement/toxicity)를 설정합니다. **toxicity score**은 0~1사이의 소수이며, 1이 가장 toxicity합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7de8e99d-60ea-48a2-bdf5-817f80b48979",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "31c576ac640544438c08ba05ad84428a",
            "15f336f874cc4744a174646efcb9914c",
            "c898c35293774090991c0710f08bd161",
            "98990cedbeed4b85bdffdae2c7f994e1",
            "c2bd040eb2154ae58b188a5ec3af77fe",
            "6fdd8c70681a48ada128c970b8d0e049",
            "f0f55bde78584a4181b8cf8392687796",
            "bd8a8aef112c490980b4df7f0d7ec951",
            "2d412e3f07d54ab685c6dd828c33f0df",
            "ee9a6ca05ff94853a23c1f16539e8b4a",
            "23f8bcc825d54fce8f9c223225b1e96d"
          ]
        },
        "id": "7de8e99d-60ea-48a2-bdf5-817f80b48979",
        "outputId": "a2822cd6-65bd-49ed-da07-fc46df448ba1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.08k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31c576ac640544438c08ba05ad84428a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "toxicity_evaluator = evaluate.load(\"toxicity\",\n",
        "                                    toxicity_model_name,\n",
        "                                    module_type=\"measurement\",\n",
        "                                    toxic_label=\"hate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "840fbc47-c5c2-469a-b5f2-6407e8f0bfde",
      "metadata": {
        "tags": [],
        "id": "840fbc47-c5c2-469a-b5f2-6407e8f0bfde"
      },
      "source": [
        "섹션 [2.2](#2.2)와 같은 문장에 대한 유해성을 검사합니다. 유해성 점수가 보상 모델에서 나온 `hate` 클래스에 대한 확률이라는 점은 당연합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5298f91c-30d1-4d17-95a7-553952ac97b5",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5298f91c-30d1-4d17-95a7-553952ac97b5",
        "outputId": "9afac2a5-0e3e-4556-a029-5269ef7644b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toxicity score for non-toxic text:\n",
            "[0.003670616541057825]\n",
            "\n",
            "Toxicity score for toxic text:\n",
            "[0.7435289621353149]\n"
          ]
        }
      ],
      "source": [
        "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
        "    non_toxic_text\n",
        "])\n",
        "\n",
        "print(\"Toxicity score for non-toxic text:\")\n",
        "print(toxicity_score[\"toxicity\"])\n",
        "\n",
        "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
        "    toxic_text\n",
        "])\n",
        "\n",
        "print(\"\\nToxicity score for toxic text:\")\n",
        "print(toxicity_score[\"toxicity\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d3e835b-14b9-4646-b1c9-c975ef3ea944",
      "metadata": {
        "tags": [],
        "id": "7d3e835b-14b9-4646-b1c9-c975ef3ea944"
      },
      "source": [
        "섹션 [2.1](#2.1)에서 나온 담화의 유해성을 계산하기 위해 evaluator을 사용합니다. test dataset (`dataset[\"test\"]`)를 섹션 [2.2](#2.2)의 frozen PEFT model에서 사용했던 것과 같은 토크나이저를 사용하여 전달합니다. 편의를 위해 일련의 작업을 `evaluate_toxicity`에 warp합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "316ab128-33ff-4a1e-8936-47bfa29d48a3",
      "metadata": {
        "tags": [],
        "id": "316ab128-33ff-4a1e-8936-47bfa29d48a3"
      },
      "outputs": [],
      "source": [
        "def evaluate_toxicity(model,\n",
        "                      toxicity_evaluator,\n",
        "                      tokenizer,\n",
        "                      dataset,\n",
        "                      num_samples):\n",
        "\n",
        "    \"\"\"\n",
        "    Preprocess the dataset and split it into train and test parts.\n",
        "\n",
        "    Parameters:\n",
        "    - model (trl model): Model to be evaluated.\n",
        "    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n",
        "    - tokenizer (transformers tokenizer): Tokenizer to be used.\n",
        "    - dataset (dataset): Input dataset for the evaluation.\n",
        "    - num_samples (int): Maximum number of samples for the evaluation.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two numpy.float64 values:\n",
        "    - mean (numpy.float64): Mean of the samples toxicity.\n",
        "    - std (numpy.float64): Standard deviation of the samples toxicity.\n",
        "    \"\"\"\n",
        "\n",
        "    max_new_tokens=100\n",
        "\n",
        "    toxicities = []\n",
        "    input_texts = []\n",
        "    for i, sample in tqdm(enumerate(dataset)):\n",
        "        input_text = sample[\"query\"]\n",
        "\n",
        "        if i > num_samples:\n",
        "            break\n",
        "\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n",
        "\n",
        "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
        "                                             top_k=0.0,\n",
        "                                             top_p=1.0,\n",
        "                                             do_sample=True)\n",
        "\n",
        "        response_token_ids = model.generate(input_ids=input_ids.to(device),\n",
        "                                            generation_config=generation_config)\n",
        "\n",
        "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
        "\n",
        "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
        "\n",
        "    # Compute mean & std using np.\n",
        "    mean = np.mean(toxicities)\n",
        "    std = np.std(toxicities)\n",
        "\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aed269c3-dbd7-4d45-bc44-c6ab6d4ae141",
      "metadata": {
        "tags": [],
        "id": "aed269c3-dbd7-4d45-bc44-c6ab6d4ae141"
      },
      "source": [
        "fine-tuning/detoxification을 하기 전 모델에 대해 유해성을 검사합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c11ede15-dc1a-4a7e-a60d-b9cadfc7d876",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c11ede15-dc1a-4a7e-a60d-b9cadfc7d876",
        "outputId": "6e2a1b33-42aa-46cc-fcbd-5e2ebb579a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11it [01:27,  7.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxicity [mean, std] before detox: [0.022183267939412457, 0.030635774486926608]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model,\n",
        "                                                                          toxicity_evaluator=toxicity_evaluator,\n",
        "                                                                          tokenizer=tokenizer,\n",
        "                                                                          dataset=dataset[\"test\"],\n",
        "                                                                          num_samples=10)\n",
        "\n",
        "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba81c90-1ac8-4403-ac1a-d4c75c6df4f0",
      "metadata": {
        "id": "1ba81c90-1ac8-4403-ac1a-d4c75c6df4f0"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Perform Fine-Tuning to Detoxify the Summaries\n",
        "\n",
        "Proximal Policy Optimization (PPO)를 사용하여 보상 모델의 RL policy를 최적화합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5516e318-8fce-4ca7-bf19-b7baf5255480",
      "metadata": {
        "id": "5516e318-8fce-4ca7-bf19-b7baf5255480"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 - Initialize `PPOTrainer`\n",
        "\n",
        "`PPOTrainer`를 사용하기 위해, collator가 필요합니다. 여기서는 딕셔너리를 특정한 방법으로 변환하는 함수입니다. 아래에서 정의하고 테스트 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8b7be1c0-382a-4fe2-8174-470f3e333e84",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b7be1c0-382a-4fe2-8174-470f3e333e84",
        "outputId": "1456491e-f9c6-4596-c0ef-5659c085f833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n",
            "Collator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"
          ]
        }
      ],
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
        "print(f'Collator input: {test_data}')\n",
        "print(f'Collator output: {collator(test_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "080c2e92-4988-4944-8353-0e1bb2048072",
      "metadata": {
        "id": "080c2e92-4988-4944-8353-0e1bb2048072"
      },
      "source": [
        "Configuration 파라미터들을 설정합니다. `ppo_model`과 토크나이저를 불러옵니다. 또한 frozen버전인 `ref_model`도 불러옵니다. 첫번쨰 모델은 최적화 되지만 두번째 모델은 시작 모델과 KL-divergence를 계산하기 위해 기능합니다. 이는 최적화된 모델이 원래 LLM과 너무 다른 결과를 도출하기 않도록 추가적인 보상 신호로 작동합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "494e09a1-9024-4f38-91eb-d73cdc3239e6",
      "metadata": {
        "tags": [],
        "id": "494e09a1-9024-4f38-91eb-d73cdc3239e6"
      },
      "outputs": [],
      "source": [
        "learning_rate=1.41e-5\n",
        "max_ppo_epochs=1\n",
        "mini_batch_size=4\n",
        "batch_size=16\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=model_name,\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=ppo_model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=tokenizer,\n",
        "                         dataset=dataset[\"train\"],\n",
        "                         data_collator=collator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ad77d2c-3800-4e15-bb38-3851d94ad374",
      "metadata": {
        "id": "7ad77d2c-3800-4e15-bb38-3851d94ad374"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 - Fine-Tune the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cac21fb-fea5-4e80-a741-87f35ae72c62",
      "metadata": {
        "id": "0cac21fb-fea5-4e80-a741-87f35ae72c62"
      },
      "source": [
        "파인튜닝 루프는 다음의 주요한 스텝을 포함합니다.\n",
        "1. policy LLM(PEFT model)로 부터 쿼리에 대한 응답을 도출합니다.\n",
        "2. hate speech RoBERTa model로 부터 응답의 감성(보상)을 도출합니다.\n",
        "3. 쿼리, 응답, 보상 triplet을 사용해 PPO로 policy를 최적화합니다.\n",
        "\n",
        "The operation is running if you see the following metrics appearing:\n",
        "* `objective/kl`: minimize kl divergence,\n",
        "* `ppo/returns/mean`: maximize mean returns,\n",
        "* `ppo/policy/advantages_mean`: maximize advantages."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01536b7e-2f0f-4986-a97c-6ecfabf518d4",
      "metadata": {
        "tags": [],
        "id": "01536b7e-2f0f-4986-a97c-6ecfabf518d4"
      },
      "source": [
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSAyMC0zMCBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4bc55397-92b8-4f61-9ec2-c8b39d5f8962",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bc55397-92b8-4f61-9ec2-c8b39d5f8962",
        "outputId": "5e536be0-e2fc-4668-9916-b163a6d5c30c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:40, 40.50s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "objective/kl: 0.0\n",
            "ppo/returns/mean: 0.6619201898574829\n",
            "ppo/policy/advantages_mean: -4.2552670720397145e-08\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r2it [01:33, 48.09s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "objective/kl: 0.019753267988562584\n",
            "ppo/returns/mean: 0.4865519404411316\n",
            "ppo/policy/advantages_mean: 2.640093921257858e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "3it [02:13, 44.11s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "objective/kl: -0.014077756553888321\n",
            "ppo/returns/mean: 0.5967159271240234\n",
            "ppo/policy/advantages_mean: -2.8326697432135006e-08\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r4it [02:46, 39.86s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "objective/kl: 0.042236704379320145\n",
            "ppo/returns/mean: 0.5421863794326782\n",
            "ppo/policy/advantages_mean: 2.631889373105878e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r5it [03:26, 39.90s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "objective/kl: -0.007229343056678772\n",
            "ppo/returns/mean: 0.6220415830612183\n",
            "ppo/policy/advantages_mean: 1.2450429132115914e-08\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r6it [03:59, 37.64s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "objective/kl: -0.02252703160047531\n",
            "ppo/returns/mean: 0.6286816596984863\n",
            "ppo/policy/advantages_mean: -4.86919127240526e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r7it [04:37, 37.69s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "objective/kl: 0.022838713601231575\n",
            "ppo/returns/mean: 0.6580877304077148\n",
            "ppo/policy/advantages_mean: 6.1684293228836395e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r8it [05:12, 36.94s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "objective/kl: -0.0021027810871601105\n",
            "ppo/returns/mean: 0.6942576169967651\n",
            "ppo/policy/advantages_mean: 2.235320550880715e-08\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r9it [05:54, 38.29s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "objective/kl: 0.01859004981815815\n",
            "ppo/returns/mean: 0.5624541640281677\n",
            "ppo/policy/advantages_mean: 3.325614272853272e-08\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [06:34, 39.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: -0.14429761469364166\n",
            "ppo/returns/mean: 0.5977291464805603\n",
            "ppo/policy/advantages_mean: 2.8035724852770727e-08\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "output_min_length = 100\n",
        "output_max_length = 400\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True\n",
        "}\n",
        "\n",
        "reward_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
        "    \"batch_size\": 16\n",
        "}\n",
        "\n",
        "max_ppo_steps = 10\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    # Break when you reach max_steps.\n",
        "    if step >= max_ppo_steps:\n",
        "        break\n",
        "\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # Get response from FLAN-T5/PEFT LLM.\n",
        "    summary_tensors = []\n",
        "\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        max_new_tokens = output_length_sampler()\n",
        "\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    # This needs to be called \"response\".\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "\n",
        "    # Compute reward outputs.\n",
        "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
        "\n",
        "    # You use the `nothate` item because this is the score for the positive `nothate` class.\n",
        "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
        "\n",
        "    # Run PPO step.\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "\n",
        "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
        "    print('-'.join('' for x in range(100)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b648cb7-89e2-40b8-9507-9c07bdfd9ebf",
      "metadata": {
        "id": "5b648cb7-89e2-40b8-9507-9c07bdfd9ebf"
      },
      "source": [
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7903f5df-a9de-41eb-b239-38bc367b5654",
      "metadata": {
        "id": "7903f5df-a9de-41eb-b239-38bc367b5654"
      },
      "source": [
        "<a name='3.3'></a>\n",
        "### 3.3 - Evaluate the Model Quantitatively\n",
        "\n",
        "PPO/PEFT을 디스크에서 불러오고, 나눠두었던 test dataset을 사용해 RL-fine-tuned model의 유해성을 검사합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3b093d43-6197-4cc0-b933-29030479a7d0",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b093d43-6197-4cc0-b933-29030479a7d0",
        "outputId": "c8f9eeeb-d663-4f12-f98e-b7af32476568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11it [00:28,  2.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxicity [mean, std] after detox: [0.029684017020785672, 0.03683596797140133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model,\n",
        "                                                                        toxicity_evaluator=toxicity_evaluator,\n",
        "                                                                        tokenizer=tokenizer,\n",
        "                                                                        dataset=dataset[\"test\"],\n",
        "                                                                        num_samples=10)\n",
        "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f42895cc-7bbf-45e1-a7c7-78ee29cd8009",
      "metadata": {
        "tags": [],
        "id": "f42895cc-7bbf-45e1-a7c7-78ee29cd8009"
      },
      "source": [
        "레퍼런스 모델(detoxification 전)과 fine-tuned model(detoxification 후)의 유해성 점수를 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "77cc3af2-6600-4673-874b-917c05247ae3",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77cc3af2-6600-4673-874b-917c05247ae3",
        "outputId": "517b30b3-2d50-4e83-e398-45275a35fb85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage improvement of toxicity score after detoxification:\n",
            "mean: -33.81%\n",
            "std: -20.24%\n"
          ]
        }
      ],
      "source": [
        "mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n",
        "std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n",
        "\n",
        "print(f'Percentage improvement of toxicity score after detoxification:')\n",
        "print(f'mean: {mean_improvement*100:.2f}%')\n",
        "print(f'std: {std_improvement*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66030581-b6f7-41d7-a7e6-2466226833be",
      "metadata": {
        "id": "66030581-b6f7-41d7-a7e6-2466226833be"
      },
      "source": [
        "<a name='3.4'></a>\n",
        "### 3.4 - Evaluate the Model Qualitatively\n",
        "\n",
        "test dataset의 예제를 몇개 검사해 봅시다. 원래 `ref_model`과 파인튜닝/detoxified `ppo_model`을 toxicity evaluator을 사용해 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "22cc8313-20ae-4d32-855e-9b2866fa3085",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22cc8313-20ae-4d32-855e-9b2866fa3085",
        "outputId": "3d2c4bbe-c8fe-4c36-a794-b4946806c710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [01:08<00:00,  3.40s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "batch_size = 20\n",
        "compare_results = {}\n",
        "\n",
        "df_batch = dataset[\"test\"][0:batch_size]\n",
        "\n",
        "compare_results[\"query\"] = df_batch[\"query\"]\n",
        "prompt_tensors = df_batch[\"input_ids\"]\n",
        "\n",
        "summary_tensors_ref = []\n",
        "summary_tensors = []\n",
        "\n",
        "# Get response from ppo and base model.\n",
        "for i in tqdm(range(batch_size)):\n",
        "    gen_len = output_length_sampler()\n",
        "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
        "\n",
        "    summary = ref_model.generate(\n",
        "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
        "        **generation_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    summary_tensors_ref.append(summary)\n",
        "\n",
        "    summary = ppo_model.generate(\n",
        "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
        "        **generation_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    summary_tensors.append(summary)\n",
        "\n",
        "# Decode responses.\n",
        "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
        "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
        "\n",
        "# Sentiment analysis of query/response pairs before/after.\n",
        "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
        "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
        "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
        "\n",
        "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
        "rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n",
        "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65b70892-4c22-4bed-9d1e-9da3f4f0c97f",
      "metadata": {
        "tags": [],
        "id": "65b70892-4c22-4bed-9d1e-9da3f4f0c97f"
      },
      "source": [
        "리뷰와 결과를 DataFrame에 담아 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e06fff0f-9dba-4517-9424-a5ebd81e8f49",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e06fff0f-9dba-4517-9424-a5ebd81e8f49",
        "outputId": "562cbdc3-07d9-4d61-eafb-40606bb888a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  query  \\\n",
              "0   Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...   \n",
              "1   Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...   \n",
              "2   Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...   \n",
              "3                                                                                                                                                                                                                                         Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: </s>   \n",
              "4           Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: </s>   \n",
              "5   Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...   \n",
              "6   Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...   \n",
              "7   Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...   \n",
              "8                       Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: </s>   \n",
              "9   Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...   \n",
              "10  Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...   \n",
              "11  Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...   \n",
              "12  Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...   \n",
              "13                                                                                                                                                                  Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: </s>   \n",
              "14  Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...   \n",
              "15                                                                                                                                                                                                                          Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: </s>   \n",
              "16  Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...   \n",
              "17  Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...   \n",
              "18                                                                                                                                                                        Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: </s>   \n",
              "19                                                                                    Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: </s>   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        response_before  \\\n",
              "0                                                                                                                                                                                                                           <pad> The contract between the collector and the shirt owner has been finalized and the positions differ from a preliminary presenter. The agreement ends with the buyer figuring out all the terms of the contract carefully. The woman offers to sign the contract on the bloody day.</s>   \n",
              "1                                                                                                                                                                                           <pad> A restaurant criticizes the current restaurant the someone recommended. According to the speaker, there aren't any official plans to open another restaurant. The food is mediocre, but the service is not as good, as is the food delivery. The speaker feels that he hasn't found the right place to try again.</s>   \n",
              "2                                                                                                                            <pad> \"An interviewer asks someone about looking for a job in an office. They take a personal test, such as doing a background check on a worker and seeing different responsibilities, such as payroll, benefits, experience, and citizenship. The employer suggests the person try to locate a job in an office and presently rate working with them on other aspects of their work.</s>   \n",
              "3                                                                                                                                                                                                          <pad> \"The conversation is about a flight going in 15 minutes and still not arriving,\" Mr. Evans tells the person. The person asks why Alice's flight hasn't been cancelled and the woman asks if they can solve the issue. The person declines, agreeing to return and finds out if there is a problem.</s>   \n",
              "4                                                                                                                                                                                                                                                                           <pad> Person1 is buying a toy car for her son but has a difficult time finding one suitable for his age. They are having a conversation about a cheaper one which is one hundred and twenty. The person decides to take the bought car.</s>   \n",
              "5                                                                                                                                                                                                                                                                                                                                               <pad> A person asks that they pay for a medical record and have the registration card provided. The person replies that he has a registration card and pays for it.</s>   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                   <pad> Li Hong apologizes to Alice's mother in distress and sends her to hospital. Alice agrees.</s>   \n",
              "7                                                                                                                      <pad> Person1 complains to their partner that it's not like they are quitting smoking and wants to stop body odorism. Daniel advises him to consider alternative solutions. They plan on attempting nicotine patches or chewing gum, but are not certain if they will achieve the maximum effect. Meanwhile, the teen wishes to divorce the person if he doesn't have the willpower to stop.</s>   \n",
              "8                                                                                                                                                                                                                                                                                                                <pad> Person1 summarizes his paper and asks his mom to proofread it before handing in it to her. Father Thomas sends them a photo of the original paper, explaining that \"its worth all the time.\"</s>   \n",
              "9                                                                                                                                                                                                                                       <pad> A search reveals that a robber has not been present and has not re-locked the house. The robber left through the door to gain entry and then left through the door, leading to illegal entry. As a warning, the police do not suspect there will be someone up there.</s>   \n",
              "10  <pad> \"Let's take a coffee break.\" (Analyst 207). \"I apologize for being so busy,\" \"Please tell me all the details and I'll show you how to complete my report.\" \"If you do not finish by the deadline, then just hold on leeet even for a minute.\" \"Would you really feel better if you just took a little break?\" \"Not a problem,\" \"I understand it, particularly since you've been sitting there for hours. You scored the 20th percent job. I am up to my neck in work.\" \"I need to finish the report by noo...   \n",
              "11                                                                                                                                                                                                                                              <pad> Person1 fills out the order of DEL or dial-up Internet for his/her online application. They recommend going with Dial-up in order to save on phone requirements. When done, Dial-up connects to the phone line, making the internet faster and more reliable.</s>   \n",
              "12                                                                                                                                                                              <pad> The passage talks about how personal computers are increasingly used for communication with the outside world. Many are relying on them to sell goods online and previously purchasing them without a physical store. To better understand this, the reader discusses the advantages and disadvantages of personal computers.</s>   \n",
              "13                                                                                                                                                                                                                                                                                   <pad> Judy asks someone what they think about Richard's firing. People liken Richard to someone being fired, on the assumption that it's not a fact, or if they humbly agree. Others liken Richard to fired, once upon a time.</s>   \n",
              "14  <pad> \"Contacting museum staff to reconfirm their flight to London needs an English speaking helper. This person asks for the flight number and that of the airline office at a hotel that has an English-speaking staff. The airline office has scheduled flights to London starting at 1 PM. The Boeing 737 is scheduled to land in London to complete the reservation. The ticket price makes this fare much lower than the original fare. ($40.43) Payment is made to the airport by International Travel Ag...   \n",
              "15                                                                                                                                                                        <pad> \"There seems to be a debate. \"Me, Amanda, are discussing a top hat, but here are suggestions. It fits me well, and I am thinking of buying a sombrero in black. Regardless of if I buy a peaked cap or a top hat, I would rather always change my clothes and wear a sombrero. Regardless of the reasons, I am concerned about it!\"</s>   \n",
              "16                                                                                                               <pad> A person is forming a rock band and intending to onboard her new instrument. However, she has left her friends and members because they haven't found a suitable singer or musician. The group have a guitar player, bass player, and guitar player. The presenter would like to audition at Person1's house this week. They are unsure of where to keep their instruments or practice them.</s>   \n",
              "17                                                                                                                                                               <pad> The Cross Bakery building is located on Broadway and Elm. At the intersection of Broadway and Elm, you have to turn around and go three blocks to Broadway. The intersection of Broadway and Elm is located at an intersection. The building is on your left hand side. However, the speaker finds some problems with this method of travel.</s>   \n",
              "18                                                                                                                                                                                                                                                                                                                                                               <pad> \"You have been asked to place the cash in a cashed account. You have chosen in 10 thousands and ten twenties, and the rest in small change.\"</s>   \n",
              "19                                                                                                                                                                                                                                                                                                                                                                           <pad> Prince Orchid Books has as a demand from Customers 10 % discount of 150 yuan a year for their merchandise, including businesses.</s>   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                   response_after  \\\n",
              "0   <pad> The contract signing is completed. The contract includes a description of the shirts to purchase, the total amount of the order, price for each piece, mode of payment, packaging, shipping time, insurance and compensation, claim and arbitration, and rights and duties for both sides. The sample 25 is the standard for others and is complete. The final draft is reviewed and asked to sign.</s>   \n",
              "1                                                                                                                                                                                                  <pad> The restaurant is new and inexperienced actually, due to the lack of a joint. The reviews are negative, however, and dinner was lacking in quality. The service was poor, and they didn't come back.</s>   \n",
              "2                                         <pad> The job center offers help in terms of looking for jobs in an office like customer service, computer skills, and other disciplines. The job center offers suitable jobs for both full-time and full-time employees, and visits local job listings from computers, print ads, and computer guides. Persons have their doubts about talking to a job counselor.</s>   \n",
              "3                                                                                                                                                                                          <pad> \"Another flight arrived fifteen minutes late for Person1 and everyone else, but her flight has not found a way in\". Someone apologized and advised them that they would ask if there is any more to be done.</s>   \n",
              "4              <pad> In a call, the secretary of the Toy Car Registry is helping Person1 purchase a toy car for his son, talking about it in detail. The difficulty of choosing a toy car is being covered by the desk clerk who concludes that the total price is three hundred dollars. If the person agrees, the car will be offered for a discount and the person is offered the correct amount of money.</s>   \n",
              "5                                                  <pad> \"Please register and pay ten yuan before the consultation.\" Customer is asked where to go to register his information and be guided through the procedure. After logging in, the consumer receives a welcome notice on their registration card. The patient is advised to follow the procedure and leave after the consultation is to the drugstore.</s>   \n",
              "6                                                                                                                                                                                                                                   <pad> Alice cannot meet with Li Hong tomorrow morning because her mother is ill and she will care for her. He apologizes and agrees to go home to visit Mrs. Brown later.</s>   \n",
              "7                                                                                                    <pad> People complain about their behavior on social media like to have a look of ashtray on their face. They suggest they use nicotine patches, chewing gum, and smoking bans instead. They try craving tobacco. However, they need a decision after explaining to the others that they want a divorce.</s>   \n",
              "8                                                                                                                                                                                                                                                                                                                 <pad> The teacher proofreads the paper, congratulating little person one on it, sent it in.</s>   \n",
              "9                                                                                                                                                                                                <pad> Persons contact the police at a crime scene and learn that someone has broken into their house in winter. The robber can't unlock the window but instead left through the door because it wasn't tied.</s>   \n",
              "10                                                                                                                                                   <pad> The two listeners share their frustrations regarding their work and are seeking a time nap. The speaker is up to their neck in work, and has to finish a report by noon. The speaker asks that the speaker take a short break to be less punctual.</s>   \n",
              "11                                                                                                                                               <pad> A customer who wants to order internet for their business requests an alternative with DEL or dial-up, which has a better connection and doesn't tie up the phone. When an internet user encounters a problem with DEL, he needs to switch to dial-up.</s>   \n",
              "12                                                                                                                                                                                              <pad> \"The Internet has expanded convenient access to the outside world, offering a more advanced means to communicate and store goods, allowing for more flexibility in purchasing a personalised computer.\"</s>   \n",
              "13                                                                                                                                                                                                                                                                                                  <pad> Judy asks someone about Richard being fired by his manager. The other person rejects it as cynical.</s>   \n",
              "14                                                                                                                                              <pad> Person1 mentions they are Confirming a flight to London with the airline, but couldn't speak English. They are taking IB 385 to London tomorrow at 1 pm and the airline offices share an English-speaking staff. They dial 35 to confirm the situation.</s>   \n",
              "15                                                                                                                                                           <pad> \"Attacks fit on most people, but peaked is the most flattering fit for Amanda,\" said one participant, tipping a blank question on the other. Asked the person for advice, the speaker advised to buy either a camouflage or black top hat.</s>   \n",
              "16                                                                                                                                                                  <pad> Person1 started a music band, with a guitarist, bass player, and keyboard player playing guitar and bass. They recruit an actual musician. They audition for another singer and equipment but they don't have enough space or room.</s>   \n",
              "17                                                                    <pad> From the Cross Bakery building, people should turn around and go three blocks to Broadway. After completing the turn from Broadway to Elm, people should go straight down the street for half a block to see the building on their left hand side. The person should also ask for a trail and show them how to get to the Bakery.</s>   \n",
              "18                                                                                                                                                   <pad> The person needs to go to a credit card office and provide them with their name and address, number, and other specific details. Person1 suggests sending the card to a credit card agency and requesting proof of papers as well as small change.</s>   \n",
              "19                                                                                                                                                                                                                                                     <pad> In China, price is 150 yuan per piece and tax can't balance it. Convert with volume discount if lot more bamkin is purchased for a 10% discount.</s>   \n",
              "\n",
              "    reward_before  reward_after  reward_diff  \n",
              "0        1.512996      3.590135     2.077139  \n",
              "1        2.388860      3.279860     0.891000  \n",
              "2        2.204305      2.962445     0.758140  \n",
              "3        2.113165      2.662541     0.549376  \n",
              "4        1.426254      1.800506     0.374251  \n",
              "5        1.461796      1.780276     0.318480  \n",
              "6        1.373268      1.596571     0.223303  \n",
              "7        1.814415      2.017397     0.202982  \n",
              "8        2.486018      2.687668     0.201650  \n",
              "9        2.023510      1.976732    -0.046778  \n",
              "10       2.108840      2.043103    -0.065736  \n",
              "11       2.380145      2.267584    -0.112561  \n",
              "12       3.001686      2.879421    -0.122265  \n",
              "13       1.496711      1.328145    -0.168567  \n",
              "14       2.332252      2.150464    -0.181788  \n",
              "15       2.652175      2.438382    -0.213793  \n",
              "16       3.003969      2.740932    -0.263037  \n",
              "17       3.545890      3.126513    -0.419377  \n",
              "18       2.062907      1.547670    -0.515237  \n",
              "19       3.692272      2.243148    -1.449124  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96420217-721d-4206-88e2-fc028c21d861\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>response_before</th>\n",
              "      <th>response_after</th>\n",
              "      <th>reward_before</th>\n",
              "      <th>reward_after</th>\n",
              "      <th>reward_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...</td>\n",
              "      <td>&lt;pad&gt; The contract between the collector and the shirt owner has been finalized and the positions differ from a preliminary presenter. The agreement ends with the buyer figuring out all the terms of the contract carefully. The woman offers to sign the contract on the bloody day.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; The contract signing is completed. The contract includes a description of the shirts to purchase, the total amount of the order, price for each piece, mode of payment, packaging, shipping time, insurance and compensation, claim and arbitration, and rights and duties for both sides. The sample 25 is the standard for others and is complete. The final draft is reviewed and asked to sign.&lt;/s&gt;</td>\n",
              "      <td>1.512996</td>\n",
              "      <td>3.590135</td>\n",
              "      <td>2.077139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...</td>\n",
              "      <td>&lt;pad&gt; A restaurant criticizes the current restaurant the someone recommended. According to the speaker, there aren't any official plans to open another restaurant. The food is mediocre, but the service is not as good, as is the food delivery. The speaker feels that he hasn't found the right place to try again.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; The restaurant is new and inexperienced actually, due to the lack of a joint. The reviews are negative, however, and dinner was lacking in quality. The service was poor, and they didn't come back.&lt;/s&gt;</td>\n",
              "      <td>2.388860</td>\n",
              "      <td>3.279860</td>\n",
              "      <td>0.891000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...</td>\n",
              "      <td>&lt;pad&gt; \"An interviewer asks someone about looking for a job in an office. They take a personal test, such as doing a background check on a worker and seeing different responsibilities, such as payroll, benefits, experience, and citizenship. The employer suggests the person try to locate a job in an office and presently rate working with them on other aspects of their work.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; The job center offers help in terms of looking for jobs in an office like customer service, computer skills, and other disciplines. The job center offers suitable jobs for both full-time and full-time employees, and visits local job listings from computers, print ads, and computer guides. Persons have their doubts about talking to a job counselor.&lt;/s&gt;</td>\n",
              "      <td>2.204305</td>\n",
              "      <td>2.962445</td>\n",
              "      <td>0.758140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: &lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; \"The conversation is about a flight going in 15 minutes and still not arriving,\" Mr. Evans tells the person. The person asks why Alice's flight hasn't been cancelled and the woman asks if they can solve the issue. The person declines, agreeing to return and finds out if there is a problem.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; \"Another flight arrived fifteen minutes late for Person1 and everyone else, but her flight has not found a way in\". Someone apologized and advised them that they would ask if there is any more to be done.&lt;/s&gt;</td>\n",
              "      <td>2.113165</td>\n",
              "      <td>2.662541</td>\n",
              "      <td>0.549376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: &lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; Person1 is buying a toy car for her son but has a difficult time finding one suitable for his age. They are having a conversation about a cheaper one which is one hundred and twenty. The person decides to take the bought car.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; In a call, the secretary of the Toy Car Registry is helping Person1 purchase a toy car for his son, talking about it in detail. The difficulty of choosing a toy car is being covered by the desk clerk who concludes that the total price is three hundred dollars. If the person agrees, the car will be offered for a discount and the person is offered the correct amount of money.&lt;/s&gt;</td>\n",
              "      <td>1.426254</td>\n",
              "      <td>1.800506</td>\n",
              "      <td>0.374251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...</td>\n",
              "      <td>&lt;pad&gt; A person asks that they pay for a medical record and have the registration card provided. The person replies that he has a registration card and pays for it.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; \"Please register and pay ten yuan before the consultation.\" Customer is asked where to go to register his information and be guided through the procedure. After logging in, the consumer receives a welcome notice on their registration card. The patient is advised to follow the procedure and leave after the consultation is to the drugstore.&lt;/s&gt;</td>\n",
              "      <td>1.461796</td>\n",
              "      <td>1.780276</td>\n",
              "      <td>0.318480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...</td>\n",
              "      <td>&lt;pad&gt; Li Hong apologizes to Alice's mother in distress and sends her to hospital. Alice agrees.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; Alice cannot meet with Li Hong tomorrow morning because her mother is ill and she will care for her. He apologizes and agrees to go home to visit Mrs. Brown later.&lt;/s&gt;</td>\n",
              "      <td>1.373268</td>\n",
              "      <td>1.596571</td>\n",
              "      <td>0.223303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...</td>\n",
              "      <td>&lt;pad&gt; Person1 complains to their partner that it's not like they are quitting smoking and wants to stop body odorism. Daniel advises him to consider alternative solutions. They plan on attempting nicotine patches or chewing gum, but are not certain if they will achieve the maximum effect. Meanwhile, the teen wishes to divorce the person if he doesn't have the willpower to stop.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; People complain about their behavior on social media like to have a look of ashtray on their face. They suggest they use nicotine patches, chewing gum, and smoking bans instead. They try craving tobacco. However, they need a decision after explaining to the others that they want a divorce.&lt;/s&gt;</td>\n",
              "      <td>1.814415</td>\n",
              "      <td>2.017397</td>\n",
              "      <td>0.202982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: &lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; Person1 summarizes his paper and asks his mom to proofread it before handing in it to her. Father Thomas sends them a photo of the original paper, explaining that \"its worth all the time.\"&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; The teacher proofreads the paper, congratulating little person one on it, sent it in.&lt;/s&gt;</td>\n",
              "      <td>2.486018</td>\n",
              "      <td>2.687668</td>\n",
              "      <td>0.201650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...</td>\n",
              "      <td>&lt;pad&gt; A search reveals that a robber has not been present and has not re-locked the house. The robber left through the door to gain entry and then left through the door, leading to illegal entry. As a warning, the police do not suspect there will be someone up there.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; Persons contact the police at a crime scene and learn that someone has broken into their house in winter. The robber can't unlock the window but instead left through the door because it wasn't tied.&lt;/s&gt;</td>\n",
              "      <td>2.023510</td>\n",
              "      <td>1.976732</td>\n",
              "      <td>-0.046778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...</td>\n",
              "      <td>&lt;pad&gt; \"Let's take a coffee break.\" (Analyst 207). \"I apologize for being so busy,\" \"Please tell me all the details and I'll show you how to complete my report.\" \"If you do not finish by the deadline, then just hold on leeet even for a minute.\" \"Would you really feel better if you just took a little break?\" \"Not a problem,\" \"I understand it, particularly since you've been sitting there for hours. You scored the 20th percent job. I am up to my neck in work.\" \"I need to finish the report by noo...</td>\n",
              "      <td>&lt;pad&gt; The two listeners share their frustrations regarding their work and are seeking a time nap. The speaker is up to their neck in work, and has to finish a report by noon. The speaker asks that the speaker take a short break to be less punctual.&lt;/s&gt;</td>\n",
              "      <td>2.108840</td>\n",
              "      <td>2.043103</td>\n",
              "      <td>-0.065736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...</td>\n",
              "      <td>&lt;pad&gt; Person1 fills out the order of DEL or dial-up Internet for his/her online application. They recommend going with Dial-up in order to save on phone requirements. When done, Dial-up connects to the phone line, making the internet faster and more reliable.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; A customer who wants to order internet for their business requests an alternative with DEL or dial-up, which has a better connection and doesn't tie up the phone. When an internet user encounters a problem with DEL, he needs to switch to dial-up.&lt;/s&gt;</td>\n",
              "      <td>2.380145</td>\n",
              "      <td>2.267584</td>\n",
              "      <td>-0.112561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...</td>\n",
              "      <td>&lt;pad&gt; The passage talks about how personal computers are increasingly used for communication with the outside world. Many are relying on them to sell goods online and previously purchasing them without a physical store. To better understand this, the reader discusses the advantages and disadvantages of personal computers.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; \"The Internet has expanded convenient access to the outside world, offering a more advanced means to communicate and store goods, allowing for more flexibility in purchasing a personalised computer.\"&lt;/s&gt;</td>\n",
              "      <td>3.001686</td>\n",
              "      <td>2.879421</td>\n",
              "      <td>-0.122265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: &lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; Judy asks someone what they think about Richard's firing. People liken Richard to someone being fired, on the assumption that it's not a fact, or if they humbly agree. Others liken Richard to fired, once upon a time.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; Judy asks someone about Richard being fired by his manager. The other person rejects it as cynical.&lt;/s&gt;</td>\n",
              "      <td>1.496711</td>\n",
              "      <td>1.328145</td>\n",
              "      <td>-0.168567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...</td>\n",
              "      <td>&lt;pad&gt; \"Contacting museum staff to reconfirm their flight to London needs an English speaking helper. This person asks for the flight number and that of the airline office at a hotel that has an English-speaking staff. The airline office has scheduled flights to London starting at 1 PM. The Boeing 737 is scheduled to land in London to complete the reservation. The ticket price makes this fare much lower than the original fare. ($40.43) Payment is made to the airport by International Travel Ag...</td>\n",
              "      <td>&lt;pad&gt; Person1 mentions they are Confirming a flight to London with the airline, but couldn't speak English. They are taking IB 385 to London tomorrow at 1 pm and the airline offices share an English-speaking staff. They dial 35 to confirm the situation.&lt;/s&gt;</td>\n",
              "      <td>2.332252</td>\n",
              "      <td>2.150464</td>\n",
              "      <td>-0.181788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: &lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; \"There seems to be a debate. \"Me, Amanda, are discussing a top hat, but here are suggestions. It fits me well, and I am thinking of buying a sombrero in black. Regardless of if I buy a peaked cap or a top hat, I would rather always change my clothes and wear a sombrero. Regardless of the reasons, I am concerned about it!\"&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; \"Attacks fit on most people, but peaked is the most flattering fit for Amanda,\" said one participant, tipping a blank question on the other. Asked the person for advice, the speaker advised to buy either a camouflage or black top hat.&lt;/s&gt;</td>\n",
              "      <td>2.652175</td>\n",
              "      <td>2.438382</td>\n",
              "      <td>-0.213793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...</td>\n",
              "      <td>&lt;pad&gt; A person is forming a rock band and intending to onboard her new instrument. However, she has left her friends and members because they haven't found a suitable singer or musician. The group have a guitar player, bass player, and guitar player. The presenter would like to audition at Person1's house this week. They are unsure of where to keep their instruments or practice them.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; Person1 started a music band, with a guitarist, bass player, and keyboard player playing guitar and bass. They recruit an actual musician. They audition for another singer and equipment but they don't have enough space or room.&lt;/s&gt;</td>\n",
              "      <td>3.003969</td>\n",
              "      <td>2.740932</td>\n",
              "      <td>-0.263037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...</td>\n",
              "      <td>&lt;pad&gt; The Cross Bakery building is located on Broadway and Elm. At the intersection of Broadway and Elm, you have to turn around and go three blocks to Broadway. The intersection of Broadway and Elm is located at an intersection. The building is on your left hand side. However, the speaker finds some problems with this method of travel.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; From the Cross Bakery building, people should turn around and go three blocks to Broadway. After completing the turn from Broadway to Elm, people should go straight down the street for half a block to see the building on their left hand side. The person should also ask for a trail and show them how to get to the Bakery.&lt;/s&gt;</td>\n",
              "      <td>3.545890</td>\n",
              "      <td>3.126513</td>\n",
              "      <td>-0.419377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: &lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; \"You have been asked to place the cash in a cashed account. You have chosen in 10 thousands and ten twenties, and the rest in small change.\"&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; The person needs to go to a credit card office and provide them with their name and address, number, and other specific details. Person1 suggests sending the card to a credit card agency and requesting proof of papers as well as small change.&lt;/s&gt;</td>\n",
              "      <td>2.062907</td>\n",
              "      <td>1.547670</td>\n",
              "      <td>-0.515237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: &lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; Prince Orchid Books has as a demand from Customers 10 % discount of 150 yuan a year for their merchandise, including businesses.&lt;/s&gt;</td>\n",
              "      <td>&lt;pad&gt; In China, price is 150 yuan per piece and tax can't balance it. Convert with volume discount if lot more bamkin is purchased for a 10% discount.&lt;/s&gt;</td>\n",
              "      <td>3.692272</td>\n",
              "      <td>2.243148</td>\n",
              "      <td>-1.449124</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96420217-721d-4206-88e2-fc028c21d861')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-96420217-721d-4206-88e2-fc028c21d861 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-96420217-721d-4206-88e2-fc028c21d861');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-697483ed-b032-45de-ac19-f0f766e8b613\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-697483ed-b032-45de-ac19-f0f766e8b613')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-697483ed-b032-45de-ac19-f0f766e8b613 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', 500)\n",
        "df_compare_results = pd.DataFrame(compare_results)\n",
        "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
        "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
        "df_compare_results_sorted"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7fb2477-f719-48de-b169-0607d355a8f6",
      "metadata": {
        "id": "e7fb2477-f719-48de-b169-0607d355a8f6"
      },
      "source": [
        "생성된 문장의 보상에 대한 mean/median 점수가 상당한 차이가 있는 것을 알 수 있습니다."
      ]
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.m5.2xlarge",
    "kernelspec": {
      "display_name": "Python 3 (Data Science 3.0)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c993d5729454e17b1b29c573bdd4659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99552bcab45a4960b6d282464aaf8b8b",
              "IPY_MODEL_502a7a256a274679bd97c5e643bd89d3",
              "IPY_MODEL_16afa8a93ab7490dbe65786f40fc9c2c"
            ],
            "layout": "IPY_MODEL_1084b824ea3d41788ddcf3f68b98f8d5"
          }
        },
        "99552bcab45a4960b6d282464aaf8b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17b5847c91ce4bc685e83fc0cbd6017b",
            "placeholder": "​",
            "style": "IPY_MODEL_a6bfdbba237b4b84a61741cdd2391e51",
            "value": "100%"
          }
        },
        "502a7a256a274679bd97c5e643bd89d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0127ba120185403d95186badd8bca8e9",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e02e24572bf43d983e32a3ec1a22c4c",
            "value": 3
          }
        },
        "16afa8a93ab7490dbe65786f40fc9c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_991c019b3a944781a4346f9bfd54d0e4",
            "placeholder": "​",
            "style": "IPY_MODEL_622778d041d445ac8a84d8d247adbc18",
            "value": " 3/3 [00:00&lt;00:00, 53.34it/s]"
          }
        },
        "1084b824ea3d41788ddcf3f68b98f8d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17b5847c91ce4bc685e83fc0cbd6017b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6bfdbba237b4b84a61741cdd2391e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0127ba120185403d95186badd8bca8e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e02e24572bf43d983e32a3ec1a22c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "991c019b3a944781a4346f9bfd54d0e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "622778d041d445ac8a84d8d247adbc18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e31dc262b56e4477b45d0c061d6eff3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28bbea489fb74c3fa0267608bfc9ced4",
              "IPY_MODEL_de204bc9054748a2870b6281a66a8685",
              "IPY_MODEL_a915ceeb8d7e400abc750507fee157ee"
            ],
            "layout": "IPY_MODEL_f27e927b4b93403e9d4b05969b41ba2e"
          }
        },
        "28bbea489fb74c3fa0267608bfc9ced4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d3850cea0ad42739638abdeb1594f52",
            "placeholder": "​",
            "style": "IPY_MODEL_92670892e37240afac79d892c02f6bf4",
            "value": "Map: 100%"
          }
        },
        "de204bc9054748a2870b6281a66a8685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eede8b2fca5a4fc9b6d8980c2bfba99c",
            "max": 10022,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f77cfeeac644c36b6e919ee34032165",
            "value": 10022
          }
        },
        "a915ceeb8d7e400abc750507fee157ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_643f0a1a9c0849cc997e176ce19d005c",
            "placeholder": "​",
            "style": "IPY_MODEL_662a47db42264feaaeb94d4f4640ae75",
            "value": " 10022/10022 [00:40&lt;00:00, 468.56 examples/s]"
          }
        },
        "f27e927b4b93403e9d4b05969b41ba2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "3d3850cea0ad42739638abdeb1594f52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92670892e37240afac79d892c02f6bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eede8b2fca5a4fc9b6d8980c2bfba99c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f77cfeeac644c36b6e919ee34032165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "643f0a1a9c0849cc997e176ce19d005c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "662a47db42264feaaeb94d4f4640ae75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31c576ac640544438c08ba05ad84428a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15f336f874cc4744a174646efcb9914c",
              "IPY_MODEL_c898c35293774090991c0710f08bd161",
              "IPY_MODEL_98990cedbeed4b85bdffdae2c7f994e1"
            ],
            "layout": "IPY_MODEL_c2bd040eb2154ae58b188a5ec3af77fe"
          }
        },
        "15f336f874cc4744a174646efcb9914c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fdd8c70681a48ada128c970b8d0e049",
            "placeholder": "​",
            "style": "IPY_MODEL_f0f55bde78584a4181b8cf8392687796",
            "value": "Downloading builder script: 100%"
          }
        },
        "c898c35293774090991c0710f08bd161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8a8aef112c490980b4df7f0d7ec951",
            "max": 6077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d412e3f07d54ab685c6dd828c33f0df",
            "value": 6077
          }
        },
        "98990cedbeed4b85bdffdae2c7f994e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee9a6ca05ff94853a23c1f16539e8b4a",
            "placeholder": "​",
            "style": "IPY_MODEL_23f8bcc825d54fce8f9c223225b1e96d",
            "value": " 6.08k/6.08k [00:00&lt;00:00, 339kB/s]"
          }
        },
        "c2bd040eb2154ae58b188a5ec3af77fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fdd8c70681a48ada128c970b8d0e049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0f55bde78584a4181b8cf8392687796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd8a8aef112c490980b4df7f0d7ec951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d412e3f07d54ab685c6dd828c33f0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee9a6ca05ff94853a23c1f16539e8b4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23f8bcc825d54fce8f9c223225b1e96d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}